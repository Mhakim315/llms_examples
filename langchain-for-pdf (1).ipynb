{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install langchain\n!pip install langchain-community\n!pip install sentence-transformers\n!pip install pdfplumber\n!pip install chromadb\n!pip install tiktoken","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport tiktoken\nfrom transformers import AutoTokenizer, AutoModel, pipeline\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.document_loaders.pdf import PDFPlumberLoader\nfrom langchain.text_splitter import CharacterTextSplitter, TokenTextSplitter\nfrom langchain.vectorstores import Chroma\nfrom langchain.llms.huggingface_pipeline import HuggingFacePipeline\nfrom langchain.chains import RetrievalQA\nfrom langchain.prompts import PromptTemplate\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T10:36:18.176954Z","iopub.execute_input":"2024-11-03T10:36:18.177360Z","iopub.status.idle":"2024-11-03T10:36:18.211780Z","shell.execute_reply.started":"2024-11-03T10:36:18.177318Z","shell.execute_reply":"2024-11-03T10:36:18.211046Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def created_sbert_mpnet():\n    model = \"sentence-transformers/all-mpnet-base-v2\"\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    return HuggingFaceEmbeddings(model_name = model, model_kwargs= {\"device\" : device})\n\ndef created_flan_t5_base(load_in_8bit):\n    model = \"google/flan-t5-xl\"\n    tokenizer = AutoTokenizer.from_pretrained(model)\n\n    return pipeline(\n        \"text2text-generation\",\n        model = model,\n        tokenizer = tokenizer,\n        max_new_tokens = 100,\n        device_map = \"auto\",\n        model_kwargs= {\"load_in_8bit\" : load_in_8bit,\n                      \"temperature\" : 0.95}\n    )\n\nembedding = created_sbert_mpnet()\nllm = created_flan_t5_base(load_in_8bit= False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T10:25:07.454746Z","iopub.execute_input":"2024-11-03T10:25:07.455590Z","iopub.status.idle":"2024-11-03T10:26:20.369877Z","shell.execute_reply.started":"2024-11-03T10:25:07.455543Z","shell.execute_reply":"2024-11-03T10:26:20.368864Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/1050973921.py:4: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n  return HuggingFaceEmbeddings(model_name = model, model_kwargs= {\"device\" : device})\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"592d4a1d005945efa978618eb2d0b4cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b31e6491bcd485cbda3d55ffd14ce07"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bcf759c3b44499aae7f219eab7cab2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"68990be99d6a4c9f97a94bb9f5283813"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"93a9db39443c4b319bd53a38a2e462f2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e28ccd4cf2b8478a85e1130e4a762196"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52328032a0264f9f87b85da0214d72ca"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35fdd704a1dc43bfbcb61f502f315d51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"599c2c7714f34177a56cb0069f685fe0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cbc2d3226b9e4607a29e9f9e76f92ce0"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c86aa338457474d8dfb679c50f9bc5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/2.54k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c2c6cc5d17164704b5ffb209c24c2cf3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"866a3c1398b84e788b6a6c6500ed7940"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.42M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f63abae2b39f448f852d43dcdc92a2cd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/2.20k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8f38e9d35d944fb1b0394788dd93cf01"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.44k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"95e3d23fd41040449597f2a3992bff75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/53.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0152a46830da4ebaa914e8798601b5fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19487731fc8f4abbb810a5937d1879fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.45G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c34d65a551a4f18a2093baddec83e74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/1.95G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"804078b5e19f47768b141aba6e6836ba"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:601: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c0e3d5e521ab43caac8a304bc892159b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"de825c3f755340b0afa655885a63be19"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"_question = \"The store-owner observed and listened to the conversation: The boy asked, Lady, Can you giveme the job of cutting your lawn?\"\n\nprompt_template = \"Answer the following question. \\n\" + _question\nresult = llm(prompt_template)\nfor res in result:\n    print(res['generated_text'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T10:29:30.869134Z","iopub.execute_input":"2024-11-03T10:29:30.869589Z","iopub.status.idle":"2024-11-03T10:29:31.539821Z","shell.execute_reply.started":"2024-11-03T10:29:30.869538Z","shell.execute_reply":"2024-11-03T10:29:31.538684Z"}},"outputs":[{"name":"stdout","text":"The store-owner was surprised.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"pdf_path = \"https://charbzaban.com/wp-content/uploads/2018/01/Charbzaban.com_.pdf\"\nloader = PDFPlumberLoader(pdf_path)\ndocuments = loader.load()\n\ntext_splitter = TokenTextSplitter(chunk_size= 100, chunk_overlap= 10)\ntexts = text_splitter.split_documents(documents)\n\npersist_directory = \"/output/kaggle/working/chromadb\"\nvectordb = Chroma.from_documents(documents = documents, embedding = embedding, persist_directory= persist_directory)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T10:39:16.411289Z","iopub.execute_input":"2024-11-03T10:39:16.412087Z","iopub.status.idle":"2024-11-03T10:40:28.528199Z","shell.execute_reply.started":"2024-11-03T10:39:16.412035Z","shell.execute_reply":"2024-11-03T10:40:28.527434Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"hf_llm  = HuggingFacePipeline(pipeline = llm)\nretriever = vectordb.as_retriever(search_kwargs = {\"k\" : 1})\nqa = RetrievalQA.from_chain_type(llm = hf_llm, chain_type = 'stuff', retriever= retriever)\n\nquestion_t5_prompt = \"\"\"\ncontext : {context}\nquestion : {question}\nanswer :\n\"\"\"\n\nQUESTION_T5_PROMPT = PromptTemplate(\n    template = question_t5_prompt, input_variables = ['context', 'question']\n)\n\nqa.combine_documents_chain.llm_chain.prompt = QUESTION_T5_PROMPT","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T10:41:53.593601Z","iopub.execute_input":"2024-11-03T10:41:53.594442Z","iopub.status.idle":"2024-11-03T10:41:53.600761Z","shell.execute_reply.started":"2024-11-03T10:41:53.594396Z","shell.execute_reply":"2024-11-03T10:41:53.599799Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"qa.combine_documents_chain.verbose = True\nqa.return_source_documents = True\nqa({'query' : _question})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-03T10:42:03.804071Z","iopub.execute_input":"2024-11-03T10:42:03.804490Z","iopub.status.idle":"2024-11-03T10:42:04.611035Z","shell.execute_reply.started":"2024-11-03T10:42:03.804449Z","shell.execute_reply":"2024-11-03T10:42:04.610117Z"}},"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/3483137872.py:3: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n  qa({'query' : _question})\n","output_type":"stream"},{"name":"stdout","text":"\n\n\u001b[1m> Entering new StuffDocumentsChain chain...\u001b[0m\n\n\u001b[1m> Finished chain.\u001b[0m\n","output_type":"stream"},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"{'query': 'The store-owner observed and listened to the conversation: The boy asked, Lady, Can you giveme the job of cutting your lawn?',\n 'result': '[ii]',\n 'source_documents': [Document(metadata={'Creator': 'Nitro Pro 9', 'ModDate': \"D:20180114004334-08'00'\", 'file_path': 'https://charbzaban.com/wp-content/uploads/2018/01/Charbzaban.com_.pdf', 'page': 12, 'source': 'https://charbzaban.com/wp-content/uploads/2018/01/Charbzaban.com_.pdf', 'total_pages': 71}, page_content='I was just checking my performance\\nA little boy went into a drug store, reached for a soda carton and pulled it over to the telephone.\\nHe climbed onto the carton so that he could reach the buttons on the phone and proceeded to\\npunch in seven digits.\\nThe store-owner observed and listened to the conversation: The boy asked, \"Lady, Can you give\\nme the job of cutting your lawn? The woman replied, \"I already have someone to cut my lawn.\"\\n\"Lady, I will cut your lawn for half the price of the person who cuts your lawn now.\" replied\\nboy. The woman responded that she was very satisfied with the person who was presently cutting\\nher lawn.\\nThe little boy found more perseverance and offered, \"Lady, I\\'ll even sweep your curb and your\\nsidewalk, so on Sunday you will have the prettiest lawn in all of Palm beach, Florida.\" Again the\\nwoman answered in the negative.\\nWith a smile on his face, the little boy replaced the receiver. The store-owner, who was listening\\nto all, walked over to the boy and said, \"Son... I like your attitude; I like that positive spirit and\\nwould like to offer you a job.\"\\nThe little boy replied, \"No thanks, I was just checking my performance with the job I already\\nhave. I am the one who is working for that lady, I was talking to!\"\\n')]}"},"metadata":{}}],"execution_count":29}]}